{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-domain embedding generator\n",
    "In this application we will use a Amazon review dataset (2018), especifically the Home and Kitchen subset (this was the most similar dataset compared to our goal with significant amount of reviews - ~6.9M), since we aim to extract aspects from refrigerator reviews. The dataset was aquired in Jianmo Ni [personal webpage](https://nijianmo.github.io/amazon/index.html).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This code sample doesn't need to be executed for the main application to work, it was necessary to create the input data structures that our model will use.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcos Wendell\\AppData\\Roaming\\Python\\Python36\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import ijson\n",
    "import string\n",
    "import codecs\n",
    "import gensim\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read the JSON file and extract the text reviews. We'll use ijson lib for this task because it creates a iterator rather than reading the whole file. Ijson parser gives a parsing object containing a triple: prefix, event and value. They can assume different caracteristics in the JSON file, but for a field: prefix contain its name; event, its type; and value, its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "with open('input/home_and_kitchen.json', 'r') as f:\n",
    "    parser = ijson.parse(f, multiple_values=True)           # using ijson since we are working with a large JSON file\n",
    "    \n",
    "    for prefix, event, value in parser:            \n",
    "        if prefix == 'reviewText':                          # we are only interested in the reviewText field\n",
    "            v = re.sub('[\\r\\t\\n]', '', value).lower()       # remove newline, tab and other unwanted chars\n",
    "            v = re.sub(r'[^\\x00-\\x7F]','', v)               # remove non-ASCII characters\n",
    "            reviews.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve only the reviews that mention some domain words\n",
    "domain = ['fridge', 'fridges', 'refrigerator', 'freezer', 'freezers', 'cooler', 'frig', 'icebox', 'icemaker', 'ice machine', 'minibar', 'refrigeration', 'refrigerate', 'cupboard', 'cupboards', 'defrost', 'microwave', 'stove', 'oven']\n",
    "reviews = [s for s in reviews if any(w in s for w in domain)]\n",
    "joined_reviews = ' '.join(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenize the sentences for the Gensim word2vec function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = codecs.open('data/tech_domain.txt', 'w', 'utf-8')\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words = [w.replace(\"'\",'') for w in stop_words]           # remove ' from stop words\n",
    "\n",
    "punct = '[' + string.punctuation.replace('-','') + ']'         # regex expression to be used in re.sub function\n",
    "\n",
    "tokenized_reviews = []\n",
    "sentences = nltk.tokenize.sent_tokenize(joined_reviews)                     # tokenize reviews into sentences\n",
    "for sent in sentences:\n",
    "    sent = re.sub(punct, '', sent)                                          # remove punctuation\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)                              # then, tokenize the sentences into words\n",
    "    tokens = [lmtzr.lemmatize(w) for w in tokens if not w in stop_words]    # remove stop words and apply lemmatization\n",
    "    if len(tokens) > 0:\n",
    "        tokenized_reviews.append(tokens)\n",
    "        out.write(' '.join(tokens)+'\\n')                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up with **more than 1.7 million** tokenized sentences, a small number compared to larger implementations, but more than enough for our purposes, and they are all somehow related to our domain. Now, we are able to create our embedding, it'll be generated based on the *CBoW approach* with *negative sampling (5)*, *window-length* of 10 context words and *word frequency threshold* equal to 5. Although most of papers working with attention-based models and in-domain embeddings use 200 words, the embedding size that we selected was **300**, because a larger embedding can fit our test set better, since we have a very small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marcos Wendell\\Miniconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "emb = gensim.models.Word2Vec(tokenized_reviews, window=5, size=200, min_count=10, workers=4)\n",
    "    \n",
    "emb.save('embeddings/refrigerator_emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "[Justifying recommendations using distantly-labeled reviews and fined-grained aspects](http://cseweb.ucsd.edu/~jmcauley/pdfs/emnlp19a.pdf) <br>\n",
    "Jianmo Ni, Jiacheng Li, Julian McAuley <br>\n",
    "Empirical Methods in Natural Language Processing (EMNLP), 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
